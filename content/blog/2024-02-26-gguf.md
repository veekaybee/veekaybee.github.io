+++
card = 'summary'
creator = '@vboykis'
date = '2024-02-26'
site = '@vboykis'
title = "GGUF, the long way around"
description = 'What are ML artifacts?'
images = ["images/gguf_parasol.png"]
+++

{{< figure  width="400" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/dbbb8ee7-f19f-44df-bce7-2612817cacd2">}}

Large language models today are [consumed in one of several ways](https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/):

1. As inference against API endpoints for proprietary hosted models on services like OpenAI, Anthropic, or one of the major cloud providers
2. As model artifacts downloaded from HuggingFace's Hub and/or trained/fine-tuned in HuggingFace format
3. As model artifacts available in a format optimized for local inference, typically GGUF, and accessed via applications like `llama.cpp` or `ollama`  
4. As [ONNX](https://onnx.ai/), a format which optimizes sharing between backend ML frameworks
 
For a side project, I recently started working with `llama.cpp`, a  `C/C++`   -based LLM inference engine targting [M-series GPUs on Apple Silicon](https://github.com/ggerganov/llama.cpp/discussions/4167).  

When you first run `llama.cpp`, you get a long log. 

{{< gist veekaybee 5236546a1b09fe9f03c42525871cf126 >}}

I started looking at the codebase to understand these logs better.  There, I encountered GGUF. [GGUF (GPT-Generated Unified Format)](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) is the file format used to serve models on `llama.cpp` and other local runners like [Llamafile, Ollama and GPT4All.](https://semaphoreci.com/blog/local-llm)

To understand how GGUF works, though, we need to first take a deep dive into machine learning models and better understand the kinds of artifacts they produce. 

# What is a machine learning model? 

Let's start by describing a machine learning model. At its simplest, a model is a file or a collection of files that contain the model architecture and weights and biases of the model generated from a training loop. In LLM land, we're generally interested in [transformer-style models and architectures.](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/) Since many transformer-style models are trained using PyTorch these days, artifacts use PyTorch's `save` implementation for serializing objects to disk.

{{< figure  width="600" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/da1b0b43-de7e-4e78-ae06-32a21a018a08">}}

For example, in a transformer, 

- **The Input**: [aggregated data](https://arxiv.org/abs/2310.20707) from the internet
- **The algorithm** 
    - Converting that data [into embeddings](https://vickiboykis.com/what_are_embeddings/)
    - Positionally encoding the embeddings to provide information about where the words are in relation to each other in the sequence
    - Creating multi-headed self-attention for each word in relation to each other word in the sequence based on initialized combinations of weights
    - Layer normalization via softmax
    - Running the resulting matrix through a feedfoward neural network
    - Projecting the output into the correct vector space for the desired task 
    - Calculating loss and then updating parameters

- **The Output**: Generally for for chat completions, the model returns the statistical likelihood that any given word completes a phrase, and if it's served as a product, it just returns the actual text output based on the highest probabilities, with numerous strategies for [how that text is selected.](https://huggingface.co/blog/how-to-generate)

{{< figure  width="600" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/cb311adb-79f3-4eea-85be-7329e4aeb111">}}

In short, we convert inputs to outputs using an equation. In addition to the model's output, we also have the model itself that is generated as an artifact of the modeling process.

{{< figure  width="600" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/67102dbc-d049-4131-997a-df1fa5378f91">}}

## What's in the model artifact

What's actually in that model artifact? Let's take a step back from transformers and build a small linear regression model in PyTorch to find out. Lucky for us, [linear regression is also](https://d2l.ai/chapter_linear-regression/index.html) a (shallow) neural network, so we can work with it in PyTorch and map our simple model to more complex ones using the same framework. 

Linear regression takes a set of numerical inputs and generates a set of numerical outputs. (In contrast to transformers, which take a set of text inputs and generates a set of text inputs and their related numerical probabilities.)

For example, let's say that we produce [artisinal hazlenut spread](https://www.greatitalianchefs.com/features/hazelnuts-piedmont) for statisticians, and want to predict how many jars of Nulltella we'll produce on any given day. Let's say we have some data available to us, and that is, how many hours of sunshine we have per day, and how many jars of Nulltella we've been able to produce every day.   

{{< figure  width="300" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/66ca00e6-1baf-4eb0-9d3d-112966beb797">}}

It turns out that we feel more inspired to produce hazlenut spread when it's sunny out, and we can clearly see this relationship between input and output in our data (we do not produce Nulltella Friday-Sunday because we prefer to spend those days writing about data serialization formats): 

```
| day_id | hours   | jars |
|--------|---------|------|
| mon    | 1       | 2    |
| tues   | 2       | 4    |
| wed    | 3       | 6    |
| thu    | 4       | 8    |
```

This is the data we'll use to train our model. We'll need to split this data into three parts: 

1. used to train our model (training data)
2. used to test the accuracy of our model (test data)
3. used to tune our hyperparameters, meta-aspects of our model like the [learning rate](https://en.wikipedia.org/wiki/Learning_rate), (validation set) during the model training phase. 

In the specific case of linear regression, there technically are no hyperparameters, although we can plausibly consider the learning rate we set in PyTorch to be one. Let’s assume we have 100 of these data points values. We split the data into train, test, and validation. A usual accepted split is to use 80% of data for training and 20% for testing/validation. We want our model to have access to as much data as possible so it learns a more accurate representation, so we leave most data for test. 

Now that we have our data, we need to write our algorithm. The equation to get output \\(y\\) from inputs \\(X\\) for linear regression is: 

$$y = \beta_0 + \beta_1 x_1 + \varepsilon $$

This tells us that the output, \\(y\\)  (the number of jars of Nulltella), can be predicted by one input variable, \\(x_1\\) (hours of sunshine) with its given weights, \\(\beta_1\\)  plus an error term \\(\varepsilon\\).

Our task is to continuously solve this equation for the difference between our actual \\(Y\\) as presented by our data and a predicted \\(\hat Y\\) based on the algorithm to find the smallest sum of squared differences, \\(\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}\\), between each point and the line. In other words, we'd like to minimize the error, because it will mean that, at each point, our \\(\hat Y\\)  is as close to our actual \\(Y\\) as we can get it, given the other points.

Here's the whole process end-to-end (with the exception of tokenization, which we only do for models where features are text and we want to do language modeling): 

{{< figure  width="600" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/9f8fb4b8-4b19-45e2-bb04-7657e447d42f">}}

## Writing the code for the model

Now, let's write some code. Let's add our actual data into the model by initializing both X and y as [PyTorch Tensor objects](https://pytorch.org/docs/stable/tensors.html).  

```python
# Hours of sunshine
X = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)

# Jars of Nocciolatella
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]], dtype=torch.float32)
```

When we train - or initially build - our model, we initialize our function with a set of values, weights, and  we iterate on those weights, usually by minimizing our cost function. 

The cost function is a function that models the difference between our model’s predicted value and the actual output based on our test data.  We’d like to minimize this cost function, and we do so with gradient descent. When we say that the model learns, we mean that we can compute what the correct inputs into a model are through an of iterative process where we feed the model data, evaluate the output, and to see if the predictions it generates improve through the process of gradient descent. We’ll know because our loss
should incrementally decrease in every training iteration.

Within code, our input data is `X`, which is a torch tensor object, and our output data is `y`. We initialize a LinearRegression which subclasses the PyTorch Module, with one linear layer, which has one input feature (sunshine) and one output feature (jars of Nulltella).

I'm going to include the code for the whole model, and then we'll talk through it piece by piece. 

```python
import torch
import torch.nn as nn
import torch.optim as optim


X = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]], dtype=torch.float32)

# Define a linear regression model and its forward pass 
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature

    def forward(self, x):
        return self.linear(x)

# Instantiate the model
model = LinearRegression()

# Inspect the model's state dictionary
print(model.state_dict())

# Define loss function and optimizer
criterion = nn.MSELoss() 
optimizer = optim.SGD(model.parameters(), lr=0.01)  

# Training loop 
num_epochs = 100
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    RMSE_loss  = torch.sqrt(loss)

    # Backward pass and optimization
    optimizer.zero_grad()  # Zero out gradients
    RMSE_loss.backward()  # Compute gradients
    optimizer.step()  # Update weights

    # Print progress
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# After training, let's test the model
test_input = torch.tensor([[5.0]], dtype=torch.float32)
predicted_output = model(test_input)
print(f'Prediction for input {test_input.item()}: {predicted_output.item()}')
```

Once we have our input data, we then initialize our model, a `LinearRegression` which subclasses [Module base class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) specifically for [linear regression.](https://github.com/pytorch/pytorch/blob/372d078f361e726bb4ac0884ac334b04c58179ef/torch/nn/modules/linear.py#L49)  A forward pass involves feeding our data into the neural network and making sure it propogagtes through all the layers. Since we only have one, we have to pass our data to a single linear layer. The forward pass is what calculates our predicted `Y`. 

```python
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature

    def forward(self, x):
        return self.linear(x)
```
We tpick how we'd like to optimize the results of the model, aka how its loss should converge. In this case, we start with `mean squared error`, and then modify it to use `RMSE`, the square root of the average squared difference between the predicted values and the actual values in a dataset. 

```python
# Define loss function and optimizer
criterion = torch.sqrl(nn.MSELoss())  # RMSE
optimizer = optim.SGD(model.parameters(), lr=0.01)
....
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    RMSE_loss  = torch.sqrt(loss)
```

Now that we've defined how we'd like the model to run, we can instantiate the model object itself: 

# Instantiating the model object

```python
model = LinearRegression()
print(model.state_dict())
```

Notice that when we instantiate a `nn.Module`, it has an attribute called the "state_dict". This is really important.  [The state dict](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) holds the information about each layer and the parameters in each layer, aka the weights and biases. 

At its heart, [it's a Python dictionary.](https://github.com/pytorch/pytorch/blob/637cf4a3f2cfdd364005681636ca885bdc4d5887/torch/nn/modules/module.py#L1842)

In this case, the implementation for LinearRegression returns an ordered dict with each layer of the network and values of those layers. Each of the values is a `Tensor`.

```python
OrderedDict([('linear.weight', tensor([[0.5408]])), ('linear.bias', tensor([-0.8195]))])

for param_tensor in model.state_dict():
    print(param_tensor, "\t", model.state_dict()[param_tensor].size())

linear.weight    torch.Size([1, 1])
linear.bias      torch.Size([1])
```

For our tiny  model, it's a small OrderedDict of tuples. You can imagine that this collection of tensors becomes extremely large and memory-intensive in a large network such as a transformer. If each parameter (each Tensor object) takes up 2 bytes in memory, [a 7-billion parameter model can take up 14GB in GPU.](https://github.com/ray-project/llm-numbers?tab=readme-ov-file#2x-number-of-parameters-typical-gpu-memory-requirements-of-an-llm-for-serving) 

We then run the forward and backward passes for the model in loops. In each step, we do a forward pass to perform the calculation, a backward pass to update the weights of our model object, and then add all that information to our model parameters.  

```python
# Define loss function and optimizer
criterion = nn.MSELoss() 
optimizer = optim.SGD(model.parameters(), lr=0.01)  

# Training loop 
num_epochs = 100
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    RMSE_loss  = torch.sqrt(loss)

    # Backward pass and optimization
    optimizer.zero_grad()  # Zero out gradients
    RMSE_loss.backward()  # Compute gradients
    optimizer.step()  # Update weights

    # Print progress
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

Once we've completed these loops, we've trained the model artifact. What we now have once we have trained a model is an in-memory object that represents the weights, biases, and metadata of that model, stored within our instance of our `LinearRegression` module. 

As we run the training loop, we can see our loss shrink, aka the actual values getting closer to the predicted:

```python
Epoch [10/100], Loss: 33.0142
Epoch [20/100], Loss: 24.2189
Epoch [30/100], Loss: 16.8170
Epoch [40/100], Loss: 10.8076
Epoch [50/100], Loss: 6.1890
Epoch [60/100], Loss: 2.9560
Epoch [70/100], Loss: 1.0853
Epoch [80/100], Loss: 0.4145
Epoch [90/100], Loss: 0.3178
Epoch [100/100], Loss: 0.2974
```

What we can also see if we print out the state dict again is that the parameters have changed as we've computed the gradients and updated the weights in the backward pass: 

```python
"""before"""
OrderedDict([('linear.weight', tensor([[-0.6216]])), ('linear.bias', tensor([0.7633]))])
linear.weight    torch.Size([1, 1])
linear.bias      torch.Size([1])
{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}
Epoch [10/100], Loss: 33.0142
Epoch [20/100], Loss: 24.2189
Epoch [30/100], Loss: 16.8170
Epoch [40/100], Loss: 10.8076
Epoch [50/100], Loss: 6.1890
Epoch [60/100], Loss: 2.9560
Epoch [70/100], Loss: 1.0853
Epoch [80/100], Loss: 0.4145
Epoch [90/100], Loss: 0.3178
Epoch [100/100], Loss: 0.2974

"""after"""
OrderedDict([('linear.weight', tensor([[1.5441]])), ('linear.bias', tensor([1.3291]))])
```
The optimizer, as we see, has its own `state_dict`, which consists of these hyperparameters we discussed before: the learning rate, the weight decay, and more:

```python
print(optimizer.state_dict())
{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}
```

Now that we have a trained model object, we can pass in new values, for example we can pass in an X value of `5` days of sunshine and see how many jars of Nulltella we expect to make. 

We do this by passing in `5` to the instantiated model object, which is now a combination of the method used to run the linear regression equation and our state dict, the weights, the current set of weights and biases to give a new predicted value. We get `9`, which pretty close to what we'd expect.

```python
test_input = torch.tensor([[5.0]], dtype=torch.float32)
predicted_output = model(test_input)
print(f'Prediction for input {test_input.item()}: {predicted_output.item()}')
Prediction for input 5.0: 9.049455642700195
```

I'm abstracting away [an enormous amount of detail](https://horace.io/brrr_intro.html) for the sake of clarity, namely the massive amount of work PyTorch does in moving this data in and out of GPUs and working with [GPU-efficient datatypes](https://developer.nvidia.com/gpugems/gpugems2/part-iv-general-purpose-computation-gpus-primer/chapter-33-implementing-efficient) for efficient computing which is a large part of the work of the library. We'll skip these for now for simplicity. 

# Serializing our objects

So far, so good. We now have stateful Python objects in-memory that convey the state of our model. But what happens when we need to persist this very large model, that we likely spent 24+ hours training, and use it again? 

This scenario is described [here](https://blog.nelhage.com/post/pickles-and-ml/), 

> Suppose a researcher is experimenting with a new deep-learning model architecture, or a variation on an existing one. Her architecture is going to have a whole bunch of configuration options and hyperparameters: the number of layers, the types of each layers, the dimensionality of various vectors, where and how to normalize activations, which nonlinearity(ies) to use, and so on. Many of the model components will be standard layers provided by the ML framework, but the researcher will be inserting bits and pieces of novel logic as well.

>Our researcher needs a way to describe a particular concrete model – a specific combination of these settings – which can be serialized and then reloaded later. She needs this for a few related reasons:

>She likely has access to a compute cluster containing GPUs or other accelerators she can use to run jobs. She needs a way to submit a model description to code running on that cluster so it can run her model on the cluster.

>While those models are training, she needs to save snapshots of their progress in such a way that they can be reloaded and resumed, in case the hardware fails or the job is preempted. Once models are trained, the researcher will want to load them again (potentially both a final snapshot, and some of the partially-trained checkpoints) in order to run evaluations and experiments on them.


What do we mean by serialization? It's the process of writing objects and classes from our programming runtime to a file. Deserialization is the process of converting data on disk to programming language objects in memory.  We now need to seralize the data into a bytestream that we can write to a file. 

{{< figure  width="600" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/47228a07-abc0-410f-b62b-48b756e3b30a">}}

Why ["serialization"](https://stackoverflow.com/questions/28552540/why-is-serialization-called-serialization)? Because back in the Old Days, data used to be stored on tape, which required bits to be in order sequentially on tape. 

# What is a file

Again, let's abstract away the GPU for simplicity and assume we're performing all these computations in CPU. Python objects [live in memory](https://docs.python.org/3/c-api/memory.html). This memory is allocated in a special private heap at the beginning of [their lifecycle](https://anvil.works/articles/pointers-in-my-python-3), in [private heap](https://stackoverflow.com/questions/10200628/heap-memory-in-c-programming) managed by the Python memory manager, with specialized heaps for different object types.

When we initialize our PyTorch model object, the operating system allocates memory through lower-level C functions, namely `malloc`, via [default memory allocators](https://docs.python.org/3/c-api/memory.html#default-memory-allocators). 

When we run our code [with tracemalloc](https://docs.python.org/3/library/tracemalloc.html), we can see how memory for PyTorch is actually allocated on CPU (keep in mind that, again, GPU operations are completely different).

```python
import tracemalloc

tracemalloc.start()

.....
pytorch
...

snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')

print("[ Top 10 ]")
for stat in top_stats[:10]:
    print(stat)

[ Top 10 ]
<frozen importlib._bootstrap_external>:672: size=21.1 MiB, count=170937, average=130 B
/Users/vicki/.pyenv/versions/3.10.0/lib/python3.10/inspect.py:2156: size=577 KiB, count=16, average=36.0 KiB
/Users/vicki/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:71: size=512 KiB, count=3, average=171 KiB
/Users/vicki/.pyenv/versions/3.10.0/lib/python3.10/dataclasses.py:434: size=410 KiB, count=4691, average=90 B
/Users/vicki/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:368: size=391 KiB, count=7122, average=56 B
/Users/vicki/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:397: size=349 KiB, count=1237, average=289 B
<frozen importlib._bootstrap_external>:128: size=213 KiB, count=1390, average=157 B
/Users/vicki/.pyenv/versions/3.10.0/lib/python3.10/functools.py:58: size=194 KiB, count=2554, average=78 B
/Users/vicki/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:373: size=136 KiB, count=2540, average=55 B
<frozen importlib._bootstrap_external>:1607: size=127 KiB, count=1133, average=115 B

```
Here, we can see we imported 170k objects from imports, and that the rest of the allocation came from allowed_functions in torch.  

# How does PyTorch write objects to files? 

We can also more explicitly see the types of these objects in memory. Among all the other objects created by PyTorch and Python system libraries, we can see our `Linear` object here, which has `state_dict` as a property. We need to serialize this object into a bytestream so we can write it to disk.  

```python
import gc
# Get all live objects
all_objects = gc.get_objects()

# Extract distinct object types
distinct_types = set(type(obj) for obj in all_objects)

# Print distinct object types
for obj_type in distinct_types:
    print(obj_type.__name__)

InputKind
KeyedRef
ReLU
Manager
_Call
UUID
Pow
Softmax
Options 
_Environ
**Linear**
CFunctionType
SafeUUID
_Real
JSONDecoder
StmtBuilder
OutDtypeOperator
MatMult
attrge
```

Pytorch [serializes objects to disk](https://pytorch.org/tutorials/beginner/saving_loading_models.html) using Python's pickle framework and wrapping the pickle `load` and `dump` methods. 

Pickle works by traversing object's hierarchy and  converting each object encountered into streamable artifacts. It does this recursively for nested representations (for example, understanding nn.Module and Linear inheriting from nn.Module) and converting these representations to byte representations so that they can be written to file. 

As an example, let's take a simple function and write it to a pickle file. 

```python
import torch.nn as nn
import torch.optim as optim
import pickle

X = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)

with open('tensors.pkl', 'wb') as f: 
    pickle.dump(X, f) 
```

when we inspect the [pickled object with pickletools](https://docs.python.org/3/library/pickletools.html), we get an idea of how the data is organized. 

We import some functions that load the data as a tensor, then the actual storage of that data, then its type. The module does the inverse when converting from pickle files to Python objects. 

```
python -m pickletools tensors.pkl
    0: \x80 PROTO      4
    2: \x95 FRAME      398
   11: \x8c SHORT_BINUNICODE 'torch._utils'
   25: \x94 MEMOIZE    (as 0)
   26: \x8c SHORT_BINUNICODE '_rebuild_tensor_v2'
   46: \x94 MEMOIZE    (as 1)
   47: \x93 STACK_GLOBAL
   48: \x94 MEMOIZE    (as 2)
   49: (    MARK
   50: \x8c     SHORT_BINUNICODE 'torch.storage'
   65: \x94     MEMOIZE    (as 3)
   66: \x8c     SHORT_BINUNICODE '_load_from_bytes'
   84: \x94     MEMOIZE    (as 4)
   85: \x93     STACK_GLOBAL
   86: \x94     MEMOIZE    (as 5)
   87: B        BINBYTES   b'\x80\x02\x8a\nl\xfc\x9cF\xf9 j\xa8P\x19.\x80\x02M\xe9\x03.\x80\x02}q\x00(X\x10\x00\x00\x00protocol_versionq\x01M\xe9\x03X\r\x00\x00\x00little_endianq\x02\x88X\n\x00\x00\x00type_sizesq\x03}q\x04(X\x05\x00\x00\x00shortq\x05K\x02X\x03\x00\x00\x00intq\x06K\x04X\x04\x00\x00\x00longq\x07K\x04uu.\x80\x02(X\x07\x00\x00\x00storageq\x00ctorch\nFloatStorage\nq\x01X\n\x00\x00\x006061074080q\x02X\x03\x00\x00\x00cpuq\x03K\x04Ntq\x04Q.\x80\x02]q\x00X\n\x00\x00\x006061074080q\x01a.\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80?\x00\x00\x00@\x00\x00@@\x00\x00\x80@'
  351: \x94     MEMOIZE    (as 6)
  352: \x85     TUPLE1
  353: \x94     MEMOIZE    (as 7)
  354: R        REDUCE
  355: \x94     MEMOIZE    (as 8)
  356: K        BININT1    0
  358: K        BININT1    4
  360: K        BININT1    1
  362: \x86     TUPLE2
  363: \x94     MEMOIZE    (as 9)
  364: K        BININT1    1
  366: K        BININT1    1
  368: \x86     TUPLE2
  369: \x94     MEMOIZE    (as 10)
  370: \x89     NEWFALSE
  371: \x8c     SHORT_BINUNICODE 'collections'
  384: \x94     MEMOIZE    (as 11)
  385: \x8c     SHORT_BINUNICODE 'OrderedDict'
  398: \x94     MEMOIZE    (as 12)
  399: \x93     STACK_GLOBAL
  400: \x94     MEMOIZE    (as 13)
  401: )        EMPTY_TUPLE
  402: R        REDUCE
  403: \x94     MEMOIZE    (as 14)
  404: t        TUPLE      (MARK at 49)
  405: \x94 MEMOIZE    (as 15)
  406: R    REDUCE
  407: \x94 MEMOIZE    (as 16)
  408: .    STOP
highest protocol among opcodes = 4

```

The main issue with pickle as a file format is that it not only bundles executable code, but that there are no checks on the code being read, and without schema guarantees, [you can pass something to the pickle that's malicious](https://nedbatchelder.com/blog/202006/pickles_nine_flaws.html), 

> The insecurity is not because pickles contain code, but because they create objects by calling constructors named in the pickle. Any callable can be used in place of your class name to construct objects. Malicious pickles will use other Python callables as the “constructors.” For example, instead of executing “models.MyObject(17)”, a dangerous pickle might execute “os.system(‘rm -rf /’)”. The unpickler can’t tell the difference between “models.MyObject” and “os.system”. Both are names it can resolve, producing something it can call. The unpickler executes either of them as directed by the pickle.'

## How Pickle works

Pickle initially worked for Pytorch-based models because it was also closely coupled to the Python ecosystem and initial ML library artifacts were not the key outputs of deep learning systems. 

> The primary output of research is knowledge, not software artifacts. Research teams write software to answer research questions and improve their/their team’s/their field’s understanding of a domain, more so than they write software in order to have software tools or solutions.

However, as the use of transformer-based models picked up after the release of the Transformer paper in 2017, so did the use of the `transformers` library, which delegates the [load](https://github.com/huggingface/transformers/blob/08cd694ef07d53f6e08e60ea6e1483dbb156924d/src/transformers/models/auto/configuration_auto.py#L1006) call to PyTorch's `load` methods, which uses pickle. 

Once practitioners started creating and uploading [pickled model artifacts to model hubs like HuggingFace](https://arxiv.org/abs/2401.13177), [machine learning model supply chain security](https://www.youtube.com/watch?v=2ethDz9KnLk&t=1103s) became an issue. 

## From pickle to safetensors

As machine learning with deep learning models trained with PyTorch exploded, these security issues came to a head, and in 2021, Trail of Bits released a post [the insecurity of pickle files.](https://github.com/trailofbits/fickling) 

Engineers at HuggingFace started developing a library known as [safetensors](https://github.com/huggingface/safetensors/tree/main) as an alternative to pickle. Safetensors was a [ developed](https://github.com/huggingface/safetensors/discussions/111) to be efficient, but, also safer and more ergonomic than pickle. 

First, `safetensors` is not bound to Python as closely as Pickle: with pickle, you can only read or write files in Python. Safetensors is compatible across languages.  Second, safetensors also limits language execution, functionality available on serialization and deserialization. Third, because the backend of safetensors is written in Rust, it enforces type safety more rigorously. Finally, safetensors was optimized for work specifically with tensors as a datatype in a way that Pickle was not. That, combined with the fact that it was wirtten in Rust [makes it really fast for reads and writes.](https://huggingface.co/docs/safetensors/en/speed.) 

After a concerted push from both Trail of Bits and EleutherAI, a security audit of safetensors was conducted and found satisfactory, [which led to HuggingFace adapting it as the default format for models on the Hub.](https://huggingface.co/blog/safetensors-security-audit) going forward. (Big thanks to [Stella and Suha](https://twitter.com/vboykis/status/1759268551129452654) for this history, and to everyone who contributed to this Twitter thread.)

## How safetensors works

How does the safetensors format work? As with most things in LLMs at the bleeding edge, the code and commit history will do most of the talking. [Let's take a look at the file spec.](https://github.com/huggingface/safetensors#yet-another-format-) 


+ 8 bytes: N, an unsigned little-endian 64-bit integer, containing the size of the header
+ N bytes: a JSON UTF-8 string representing the header.
    The header data MUST begin with a { character (0x7B).
    The header data MAY be trailing padded with whitespace (0x20).
    The header is a dict like {"TENSOR_NAME": {"dtype": "F16", "shape": [1, 16, 256], "data_offsets": [BEGIN, END]}, "NEXT_TENSOR_NAME": {...}, ...},
        data_offsets point to the tensor data relative to the beginning of the byte buffer (i.e. not an absolute position in the file), with BEGIN as the starting offset and END as the one-past offset (so total tensor byte size = END - BEGIN).
    A special key __metadata__ is allowed to contain free form string-to-string map. Arbitrary JSON is not allowed, all values must be strings.
+ Rest of the file: byte-buffer.

This is different than `state_dict` and `pickle` file specifications, but the addition of safetensors follows the natural evolution from Python objects, to full-fledged file format. 

A file is a way of storing our data generated from programming language objects, in bytes on disk. In looking at different file format specs ([Arrow](https://arrow.apache.org/docs/format/CDataInterface.html),[Parquet](https://parquet.apache.org/docs/file-format/), [protobuf](https://protobuf.dev/)), we'll start to notice some patterns around how they're laid out. 

1. In the file, we need some indicator that this is a type of file "X". Usually this is represented by a [****magic byte**.](https://en.wikipedia.org/wiki/List_of_file_signatures) 
2. Then, there is a **header** that represents the metadata of the file (In the case of machine learning, how many layers we have, the learning rate, and other aspects. )
3. The actual **data**. (In the case of machine learning files, the tensors)
4. We then need a **spec** that tells us what to expect in a file as we read it and what kinds of data types are in the file and how they're represented as bytes. Essentially, documentation for the file's layout and API so that we can program a file reader against it.
5. One feature the file spec usually tells us is whether data is little or big-endian, that is - whether we store the largest number first or last. This becomes important as we expect files to be read on systems with different default byte layouts.
6. We then implement code that reads and writes to that filespec specifically. 

One thing we start to notice from having looked at statedicts and pickle files before, is that machine learning data storage follow a pattern: we need to store:
 1. a large collection of vectors, 
 2. metadata about those vectors and 
 3. hyperparameters

We then need to be able to instantiate model objects that we can hydrate (fill) with that data and run model operations on. 

As an example for safetensors from the documentation: We start with a Python dictionary, aka a state dict, save, and load the file.  

```python
import torch
from safetensors import safe_open
from safetensors.torch import save_file

tensors = {
   "weight1": torch.zeros((1024, 1024)),
   "weight2": torch.zeros((1024, 1024))
}
save_file(tensors, "model.safetensors")

tensors = {}
with safe_open("model.safetensors", framework="pt", device="cpu") as f:
   for key in f.keys():
       tensors[key] = f.get_tensor(key)
```

we use the save_file(model.state_dict(), 'my_model.st') method to render the file to safetensors 

In the conversion process from pickle to safetensors, we also start [with the state dict.](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) 

Safetensors quickly became the leading format for sharing model weights and architectures to use in further fine-tuning, and in some cases, inference 

## Checkpoint files

We've so far taken a look at simple `state_dict` files and single `safetensors` files. But if you're training a long-running model, you'll likely have more than just weights and biases to save, and you want to save your state every so often so you can revert if you start to see issues in your trianing run. [PyTorch has checkpoints](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html). A checkpoint is a file that has a model `state_dict`, but also   

>the optimizer’s state_dict, as this contains buffers and parameters that are updated as the model trains. Other items that you may want to save are the epoch you left off on, the latest recorded training loss, external torch.nn.Embedding layers, and more. This is also saved as a Dictionary and pickled, then unpickled when you need it. All of this is also saved to a dictionary, the `optimizer_state_dict`, distinct from the `model_state_dict`. 

```python
# Additional information
EPOCH = 5
PATH = "model.pt"
LOSS = 0.4

torch.save({
            'epoch': EPOCH,
            'model_state_dict': net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': LOSS,
            }, PATH)
```
In addition, most large language models also now include accompanying files like tokenizers, and on HuggingFace, metadata, etc. So if you're working with PyTorch models as artifacts generated via the Transformers library, you'll get a repo [that looks like this](https://huggingface.co/mistralai/Mistral-7B-v0.1/tree/main). 


## GGML

As work to migrate from pickle to safetensors was ongoing for [generalized model fine-tuning and inference](https://www.reddit.com/r/LocalLLaMA/comments/1ayd4xr/for_those_who_dont_know_what_different_model/), Apple Silicon [continued to get a lot better.](https://appleinsider.com/articles/23/12/13/apple-silicon-m3-pro-blows-away-nvidia-rtx-4090-gpu-in-ai-benchmark). As a result, people started bringing   modeling work and inference from large GPU-based computing clusters, to local and on-edge devices.

Georgi Gerganov's project to make OpenAI's Whisper run locally with [Whisper.cpp.](https://github.com/ggerganov/whisper.cpp) was a success and the catalyst for later projects.  The combination of the release of [Llama-2 as a mostly open-source model](https://about.fb.com/news/2023/07/llama-2/), combined with the rise of model compression techniques like [LoRA](https://huggingface.co/docs/peft/main/en/developer_guides/lora), large language models, which were typically only accessible on lab or industry-grade GPU hardware (inspie of the small CPU-based examples we've run here), also acted as a catalyst for thinking about working with and running personalized models locally. 

Based on the interest and success of `whisper.cpp`, Gerganov created [llama.cpp](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022), a package for working with Llama model weights, originaly in pickle format, in GGML format, for local inference. 

GGML was initialy both a library and a complementary format created specifically for on-edge inference for whisper. You can also [perform fine-tuning](https://www.reddit.com/r/LocalLLaMA/comments/15y9m64/fine_tuningggml_quantiziation_on_apple_silicon/) with it, but generally it's used to read models trained on PyTorch in GPU Linux-based environments and converted to GGML to run on Apple Silicon. 

As an example, here is script for [GGML](https://github.com/ggerganov/ggml) which [converts PyTorch GPT-2 checkpoints](https://github.com/ggerganov/ggml/blob/master/examples/gpt-2/convert-ckpt-to-ggml.py) to  the correct format, [read as a `.bin` file.](https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L64). The files are [downloaded from OpenAI](https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/download-model.sh#L41C64-L41C131). 

[The resulting GGML file compresses all of these into one and contains](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022): 

+ a magic number with an [optional version number](https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L91)

+ [model-specific hyperparameters](https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L92), including
    metadata about the model, such as the number of layers, the number of heads, etc.
    a ftype that describes the type of the majority of the tensors,
        for GGML files, the quantization version is encoded in the ftype divided by 1000
+ an embedded vocabulary, which is a list of strings with length prepended. 
+ finally, a [list of tensors](https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L137) with their length-prepended name, type, and  tensor data

There are several elements that make GGML more efficient for local inference than checkpoint files. First, [it makes use of 16-bit floating point representations](https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/src/ggml-impl.h#L45) of model weights. Generally, `torch` initializes floating point datatypes in [32-bit floats by default](https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html). 16-bit, or half precision means that model weights use [50% less memory](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) at compute and inference time without significant loss in model accuracy. Other architectural choices include using C, which offers [more efficient memory allocation than Python](https://www.interviewbit.com/blog/difference-between-c-and-python/). And finally, GGML was built [optimized for Silicon.](https://developer.apple.com/documentation/apple-silicon/tuning-your-code-s-performance-for-apple-silicon) 

Unfortunately, in its move to efficiency, GGML contained [a number of breaking changes](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#drawbacks) that created issues for users. 

The largest one was that, since everything, both data and metadata and hyperparameters, was written into the same file, if a model added hyperparameters, it would break backward compatibility that the new file couldn't pick up. Additionally, no model architecture metadata is present in the file, and each architecture required its own conversion script. All of this led to brittle performance and the creation of [GGUF.](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#gguf) 

GGUF has the same type of layout as GGML, with metadata and tensor data in a single file, but in addition is also designed to be backwards-compatible. The key difference is that previously instead of a list of values for the hyperparameters, the new file format uses a key-value lookup tables which accomodate shifting values.  

The intiution we spent building up around how machine learning models work and file formats are laid out now allows us to understand the [GGUF format.](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#file-structure)

First, we know that GGUF models are little-endian by default for specific architectures, which we remember is when the least significant bytes come first and is optimized for different computer hardware architectures.

Then, we have `gguf_header_t`, which is the header 

It includes the magic byte that tells us this is a GGUF file: 
```
Must be `GGUF` at the byte level: `0x47` `0x47` `0x55` `0x46`. 
```

as well as the key-value pairs:

```
// The metadata key-value pairs.
    gguf_metadata_kv_t metadata_kv[metadata_kv_count];
```

This file format also offers versioning, in this case we see this is version 3 of the file format.

```
// Must be `3` for version described in this spec, which introduces big-endian support.
    //
    // This version should only be increased for structural changes to the format.
```

Then, we have the tensors

```
gguf_tensor_info_t
```

The entire file looks like this, and when we work with readers like `llama.cpp` and `ollama`, they take this spec and write code to open these files and read them. 


{{< figure  width="600" src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/2edc2cb2-f14c-4fa6-a081-3730fa7d4dcb">}}


## Conclusion

We've been on a whirlwind adventure to build up our intuition of how machine learning models work, what artifacts they produce, how the machine learning artifact storage story has changed over the past couple years, and finally ended up in GGUF's documentation to better understand the log that is presented to us when we perform local inference on artifacts in GGUF. Hope this is helpful, and good luck!



